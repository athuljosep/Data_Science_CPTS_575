---
title: "Assignment 5.b"
author: |
  Athul Jose P \\ 
  11867566
dept: "School of Electrical Engineering and Computer Science"
uni: "Washington State University"
subtitle: "CptS 575 Data Science"
geometry: margin=1in
fontsize: 12pt
format: pdf
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}\Huge\bfseries}
  - \posttitle{\par\end{center}\vfill}
  - \preauthor{\begin{center}\Large}
  - \postauthor{\par\end{center}\vfill}
  - \predate{\begin{center}\Large}
  - \postdate{\par\end{center}\vfill}
  - \usepackage{fancyhdr}
  - \fancypagestyle{plain}{
      \fancyhf{}
      \renewcommand{\headrulewidth}{0pt}
      \renewcommand{\footrulewidth}{0pt}
      \fancyfoot[C]{\thepage}
    }
  - \pagestyle{empty}
---

\begin{center}
{\Large Athul Jose P \\ 11867566} \\[0.5cm]
{\large School of Electrical Engineering and Computer Science} \\[0.5cm]
{\large Washington State University} \\[0.5cm]
{\large CptS 575 Data Science} \\[0.5cm]
\end{center}

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
```

\newpage

### **1.**

```{r}
# Load necessary libraries
library(readr)       # For reading the CSV file
library(tm)          # For text mining and preprocessing
library(SnowballC)   # For stemming
library(tokenizers)  # For tokenization
library(quanteda)    # For creating Document-Term Matrix

# Step 1: Load the Dataset
data <- read.csv("bbc.csv")

# Check the structure of the dataset
str(data)

# Step 2: Preprocessing the Text
# Convert the text to lowercase, remove punctuation and numbers, and perform stemming
corpus <- Corpus(VectorSource(data$text))  # Create corpus

corpus <- tm_map(corpus, content_transformer(tolower))    # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation)               # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)                   # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stop words
corpus <- tm_map(corpus, stemDocument)                    # Perform stemming

# Step 3: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)

# Check the dimensions of the matrix
print(dim(dtm))

# Step 4: Remove low-frequency words (15% least frequent terms)
term_frequency <- colSums(as.matrix(dtm))
sorted_terms <- sort(term_frequency, decreasing = TRUE)

# Keep only the top 85% of terms
threshold <- quantile(sorted_terms, 0.85)
dtm_filtered <- dtm[, which(term_frequency >= threshold)]

# Step 5: Display words from the 2205th article with frequency >= 4
article_2205 <- as.matrix(dtm_filtered[2205, ])
feature_vector <- article_2205[article_2205 >= 4]
print(feature_vector)

```

\newpage

### **2.**

```{r}
# Load necessary libraries
library(readr)            # For reading data
library(tm)               # For text preprocessing
library(SnowballC)        # For stemming
library(caret)            # For feature selection, train/test split, and evaluation
library(naivebayes)       # For Multinomial Naive Bayes
library(nnet)             # For Multinomial Logistic Regression
library(dplyr)            # For data manipulation

# Step 4: Feature Selection - Remove Sparse Terms
dtm <- removeSparseTerms(dtm, 0.99)  # Keep terms that appear in at least 1% of documents

# Convert the DTM to a data frame and add the category labels
df <- as.data.frame(as.matrix(dtm))
df$category <- data$category

# Step 5: Split Data into Training and Test Sets (80-20 Split)
set.seed(123)  # Set seed for reproducibility
train_index <- createDataPartition(df$category, p = 0.8, list = FALSE)  # Stratified split
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# Step 6: Train a Multinomial Naive Bayes Classifier
nb_model <- multinomial_naive_bayes(as.matrix(train_data[, -ncol(train_data)]), 
                                    train_data$category)

# Predict the categories for the test data
nb_predictions <- predict(nb_model, as.matrix(test_data[, -ncol(test_data)]))

# Step 7: Ensure Predictions and Actual Categories are Factors with the Same Levels
# Convert both predictions and actual categories to factors with matching levels
nb_predictions <- factor(nb_predictions, levels = levels(test_data$category))
test_data$category <- factor(test_data$category)

# Evaluate the Naive Bayes Model with a Confusion Matrix
nb_conf_matrix <- confusionMatrix(nb_predictions, test_data$category)
print(nb_conf_matrix)

# Calculate Precision and Recall for Naive Bayes
nb_results <- data.frame(nb_conf_matrix$byClass[, c("Precision", "Recall")])
print(nb_results)

# Step 8: Train a Multinomial Logistic Regression Classifier
log_reg_model <- multinom(category ~ ., data = train_data)

# Predict the categories for the test data
log_reg_predictions <- predict(log_reg_model, newdata = test_data)

# Step 9: Evaluate the Logistic Regression Model
log_reg_conf_matrix <- confusionMatrix(log_reg_predictions, test_data$category)
print(log_reg_conf_matrix)

# Calculate Precision and Recall for Logistic Regression
log_reg_results <- data.frame(log_reg_conf_matrix$byClass[, c("Precision", "Recall")])
print(log_reg_results)

# Step 10: Compare Results
print("Naive Bayes - Precision and Recall:")
print(nb_results)

print("Logistic Regression - Precision and Recall:")
print(log_reg_results)

```

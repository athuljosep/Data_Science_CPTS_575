---
title: "Assignment 5.b"
author: |
  Athul Jose P \\ 
  11867566
dept: "School of Electrical Engineering and Computer Science"
uni: "Washington State University"
subtitle: "CptS 575 Data Science"
geometry: margin=1in
fontsize: 12pt
format: pdf
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}\Huge\bfseries}
  - \posttitle{\par\end{center}\vfill}
  - \preauthor{\begin{center}\Large}
  - \postauthor{\par\end{center}\vfill}
  - \predate{\begin{center}\Large}
  - \postdate{\par\end{center}\vfill}
  - \usepackage{fancyhdr}
  - \fancypagestyle{plain}{
      \fancyhf{}
      \renewcommand{\headrulewidth}{0pt}
      \renewcommand{\footrulewidth}{0pt}
      \fancyfoot[C]{\thepage}
---

\begin{center}
{\Large Athul Jose P \\ 11867566} \\[0.5cm]
{\large School of Electrical Engineering and Computer Science} \\[0.5cm]
{\large Washington State University} \\[0.5cm]
{\large CptS 575 Data Science} \\[0.5cm]
\end{center}

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
```

\newpage

### **1.**

```{r}
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)

# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)

# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus, 
                what = "word", 
                remove_punct = TRUE, 
                remove_numbers = TRUE) %>%
                tokens_tolower() %>%
                tokens_remove(stopwords("english")) %>%
                tokens_wordstem(language = "en")

dtm <- dfm(corpus)

# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]

# 2205th article
article_vector <- as.matrix(dtm_thres[2205, ])
article_vector_filtered <- article_vector[article_vector >= 4]
print(article_vector_filtered)
```

\newpage

### **2.**

```{r}
# loading libraries
library(quanteda)
library(caret)
library(naivebayes)
library(dplyr)
library(glmnet)

dtm_df <- as.data.frame(as.matrix(dtm_thres))

# dtm reduction using variance
feature_variances <- apply(dtm_df, 2, var)
variance_threshold <- 0.1
dtm_reduced <- dtm_df[, feature_variances > variance_threshold]

# print dimensions
print(dim(dtm_df))
print(dim(dtm_reduced))
```

```{r}
# splitting into train and test
data_split <- cbind(category = data$category, dtm_reduced)
set.seed(123)
train_index <- createDataPartition(data_split$category, p = 0.8, list = FALSE)
train_data <- data_split[train_index, ]
test_data <- data_split[-train_index, ]

# Separate x & y
train_x <- as.matrix(train_data[, -1])
train_y <- as.factor(train_data$category)
test_x <- as.matrix(test_data[, -1])
```

```{r}
# multinomial naive bayes
nb_model <- multinomial_naive_bayes(train_x, train_y)
nb_predictions <- predict(nb_model, newdata = test_x)
nb_predictions <- factor(nb_predictions, levels = levels(train_y))
test_data$category <- factor(test_data$category, levels = levels(train_y))

# confusion matrix
conf_matrix <- confusionMatrix(nb_predictions, test_data$category)
print(conf_matrix)

# Precision and Recall
conf_table <- conf_matrix$table
precision <- diag(conf_table) / rowSums(conf_table)
recall <- diag(conf_table) / colSums(conf_table)
print("Precision by class:")
print(precision)
print("Recall by class:")
print(recall)
```

```{r}
# multinomial logistic regression
train_y_numeric <- as.numeric(as.factor(train_y)) - 1
log_reg_model <- cv.glmnet(as.matrix(train_x), train_y_numeric, family = "multinomial", alpha = 0)
log_reg_predictions <- predict(log_reg_model, newx = as.matrix(test_x), s = "lambda.min", type = "class")
log_reg_predictions <- factor(log_reg_predictions, labels = levels(train_y))
test_y <- factor(test_data$category, levels = levels(train_y))

# confusion matrix
conf_matrix_log_reg <- confusionMatrix(log_reg_predictions, test_y)
print(conf_matrix_log_reg)

# Precision and Recall
conf_table_log_reg <- conf_matrix_log_reg$table
precision_log_reg <- diag(conf_table_log_reg) / rowSums(conf_table_log_reg)
recall_log_reg <- diag(conf_table_log_reg) / colSums(conf_table_log_reg)
print("Precision by class:")
print(precision_log_reg)
print("Recall by class:")
print(recall_log_reg)
```

The comparison between Multinomial Naïve Bayes and Multinomial Logistic Regression reveals that Logistic Regression provides a marginally better overall fit for this dataset. Logistic Regression achieves a higher accuracy (96.4% vs. 95.7%) and kappa (0.9548 vs. 0.9463) compared to Naïve Bayes, suggesting a stronger agreement with the true classifications. Examining class-specific metrics, Logistic Regression shows higher sensitivity across most categories, particularly for “Business” and “Entertainment,” which indicates a greater ability to correctly identify positive instances in these classes. Naïve Bayes, however, slightly outperforms Logistic Regression in the “Politics” category, where it has a sensitivity of 0.9880 compared to 0.9398 for Logistic Regression. In terms of specificity, Logistic Regression consistently surpasses Naïve Bayes, particularly in the "Politics" and "Tech" categories, which suggests that Logistic Regression is better at correctly identifying negative instances for these classes.

Precision and recall scores reinforce these trends. Logistic Regression demonstrates higher precision in “Politics” and “Tech,” while Naïve Bayes provides slightly better precision for “Entertainment” and “Sport.” Both models achieve excellent recall, with Logistic Regression slightly outperforming Naïve Bayes in the “Entertainment” and “Tech” categories. Both models, however, achieve perfect recall for the “Sport” category, indicating that this class is well-distinguished by both approaches. In summary, Multinomial Logistic Regression provides a slight advantage in terms of accuracy, specificity, and precision, making it a slightly more accurate model for this dataset. Nevertheless, Multinomial Naïve Bayes remains a strong contender and may be preferred when computational efficiency is a priority, as it is less resource-intensive than Logistic Regression.

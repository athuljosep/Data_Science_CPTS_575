model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
summary(model)$coefficients[2, "Pr(>|t|)"]
}
# Get p-values from simple regression for all predictors
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)
# Create a data frame for plotting
comparison_df <- data.frame(
Predictor = predictors,
Simple_Regression = simple_pvalues,
Multiple_Regression = multiple_pvalues
)
# Reshape data for plotting without melt()
comparison_df_long <- data.frame(
Predictor = rep(comparison_df$Predictor, 2),
Model = c(rep("Simple Regression", length(predictors)),
rep("Multiple Regression", length(predictors))),
p_value = c(comparison_df$Simple_Regression, comparison_df$Multiple_Regression)
)
# Create the bar plot
ggplot(comparison_df_long, aes(x = Predictor, y = p_value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
scale_y_log10() +  # Use log scale for better visualization
labs(title = "Comparison of P-values: Simple vs. Multiple Regression",
y = "P-value (log scale)", x = "Predictor") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Load necessary libraries
library(MASS)
library(ggplot2)
# Load the Boston dataset
data("Boston")
# Fit the multiple regression model with all predictors
model_full <- lm(crim ~ ., data = Boston)
multiple_pvalues <- summary(model_full)$coefficients[-1, "Pr(>|t|)"]  # Exclude intercept
# Function to get p-values from simple linear regression models
get_simple_pvalue <- function(predictor) {
model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
summary(model)$coefficients[2, "Pr(>|t|)"]
}
# Get p-values from simple regression for all predictors
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)
# Create a data frame for plotting
comparison_df <- data.frame(
Predictor = predictors,
Simple_Regression = simple_pvalues,
Multiple_Regression = multiple_pvalues
)
# Create a scatter plot for p-values comparison
ggplot(comparison_df, aes(x = Simple_Regression, y = Multiple_Regression, label = Predictor)) +
geom_point(color = "blue", size = 3) +
geom_text(vjust = -0.5, size = 3) +
scale_x_log10() +  # Log scale for better visualization of small p-values
scale_y_log10() +  # Log scale for y-axis as well
labs(
title = "Comparison of P-values: Simple vs. Multiple Regression",
x = "P-value (Simple Regression, log scale)",
y = "P-value (Multiple Regression, log scale)"
) +
theme_minimal()
library(ggplot2)
# exclude intercept
multiple_pvalues <- summary(model_full)$coefficients[-1, "Pr(>|t|)"]
# get p values from simple
get_simple_pvalue <- function(predictor) {
model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
summary(model)$coefficients[2, "Pr(>|t|)"]
}
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)
# collect data
comparison_df <- data.frame(
Predictor = predictors,
Simple_Regression = simple_pvalues,
Multiple_Regression = multiple_pvalues
)
# df for plotting
comparison_df_long <- data.frame(
Predictor = rep(comparison_df$Predictor, 2),
Model = c(rep("Simple Regression", length(predictors)),
rep("Multiple Regression", length(predictors))),
p_value = c(comparison_df$Simple_Regression, comparison_df$Multiple_Regression)
)
# plotting bar plot
ggplot(comparison_df_long, aes(x = Predictor, y = p_value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
scale_y_log10() +
labs(title = "Comparison of P-values: Simple vs. Multiple Regression",
y = "P-value (log scale)", x = "Predictor") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(ggplot2)
# exclude intercept
multiple_pvalues <- summary(model_full)$coefficients[-1, "Pr(>|t|)"]
# get p values from simple
get_simple_pvalue <- function(predictor) {
model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
summary(model)$coefficients[2, "Pr(>|t|)"]
}
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)
# collect data
comparison_df <- data.frame(
Predictor = predictors,
Simple_Regression = simple_pvalues,
Multiple_Regression = multiple_pvalues
)
# df for plotting
comparison_df_long <- data.frame(
Predictor = rep(comparison_df$Predictor, 2),
Model = c(rep("Simple Regression", length(predictors)),
rep("Multiple Regression", length(predictors))),
p_value = c(comparison_df$Simple_Regression, comparison_df$Multiple_Regression)
)
# plotting bar plot
ggplot(comparison_df_long, aes(x = Predictor, y = p_value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
scale_y_log10() +
labs(title = "Comparison of P-values: Simple vs. Multiple Regression",
y = "P-value (log scale)", x = "Predictor") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(ggplot2)
# exclude intercept
multiple_pvalues <- summary(model_full)$coefficients[-1, "Pr(>|t|)"]
# get p values from simple
get_simple_pvalue <- function(predictor) {
model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
summary(model)$coefficients[2, "Pr(>|t|)"]
}
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)
# collect data
comparison_df <- data.frame(
Predictor = predictors,
Simple_Regression = simple_pvalues,
Multiple_Regression = multiple_pvalues
)
# df for plotting
comparison_df_long <- data.frame(
Predictor = rep(comparison_df$Predictor, 2),
Model = c(rep("Simple Regression", length(predictors)),
rep("Multiple Regression", length(predictors))),
p_value = c(comparison_df$Simple_Regression, comparison_df$Multiple_Regression)
)
# plotting bar plot
ggplot(comparison_df_long, aes(x = Predictor, y = p_value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
scale_y_log10() +
labs(title = "Comparison of P-values: Simple vs. Multiple Regression",
y = "P-value (log scale)", x = "Predictor") +
#theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(ggplot2)
# exclude intercept
multiple_pvalues <- summary(model_full)$coefficients[-1, "Pr(>|t|)"]
# get p values from simple
get_simple_pvalue <- function(predictor) {
model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
summary(model)$coefficients[2, "Pr(>|t|)"]
}
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)
# collect data
comparison_df <- data.frame(
Predictor = predictors,
Simple_Regression = simple_pvalues,
Multiple_Regression = multiple_pvalues
)
# df for plotting
comparison_df_long <- data.frame(
Predictor = rep(comparison_df$Predictor, 2),
Model = c(rep("Simple Regression", length(predictors)),
rep("Multiple Regression", length(predictors))),
p_value = c(comparison_df$Simple_Regression, comparison_df$Multiple_Regression)
)
# plotting bar plot
ggplot(comparison_df_long, aes(x = Predictor, y = p_value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
scale_y_log10() +
labs(title = "Comparison of P-values: Simple vs. Multiple Regression",
y = "P-value (log scale)", x = "Predictor")
#theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(ggplot2)
# exclude intercept
multiple_pvalues <- summary(model_full)$coefficients[-1, "Pr(>|t|)"]
# get p values from simple
get_simple_pvalue <- function(predictor) {
model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
summary(model)$coefficients[2, "Pr(>|t|)"]
}
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)
# collect data
comparison_df <- data.frame(
Predictor = predictors,
Simple_Regression = simple_pvalues,
Multiple_Regression = multiple_pvalues
)
# df for plotting
comparison_df_long <- data.frame(
Predictor = rep(comparison_df$Predictor, 2),
Model = c(rep("Simple Regression", length(predictors)),
rep("Multiple Regression", length(predictors))),
p_value = c(comparison_df$Simple_Regression, comparison_df$Multiple_Regression)
)
# plotting bar plot
ggplot(comparison_df_long, aes(x = Predictor, y = p_value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
scale_y_log10() +
labs(title = "Comparison of P-values: Simple vs. Multiple Regression",
y = "P-value (log scale)", x = "Predictor")
library(ggplot2)
# exclude intercept
multiple_pvalues <- summary(model_full)$coefficients[-1, "Pr(>|t|)"]
# get p values from simple
get_simple_pvalue <- function(predictor) {
model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
summary(model)$coefficients[2, "Pr(>|t|)"]
}
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)
# collect data
comparison_df <- data.frame(
Predictor = predictors,
Simple_Regression = simple_pvalues,
Multiple_Regression = multiple_pvalues
)
# df for plotting
comparison_df_long <- data.frame(
Predictor = rep(comparison_df$Predictor, 2),
Model = c(rep("Simple Regression", length(predictors)),
rep("Multiple Regression", length(predictors))),
p_value = c(comparison_df$Simple_Regression, comparison_df$Multiple_Regression)
)
# plotting bar plot
ggplot(comparison_df_long, aes(x = Predictor, y = p_value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
scale_y_log10() +
labs(title = "Comparison of P-values: Simple vs. Multiple Regression",
y = "P-value (log scale)", x = "Predictor")
# polynomial regression model for 'age' with degree 3
model_age <- lm(crim ~ poly(age, 3), data = Boston)
summary(model_age)
# polynomial regression model for 'tax' with degree 3
model_tax <- lm(crim ~ poly(tax, 3), data = Boston)
summary(model_tax)
# Visualize the non-linear relationship for 'age'
ggplot(Boston, aes(x = age, y = crim)) +
geom_point(alpha = 0.4) +
stat_smooth(method = "lm", formula = y ~ poly(x, 3), color = "blue", se = FALSE) +
labs(title = "Non-linear Relationship between Age and Crime Rate",
x = "Age", y = "Crime Rate")
# Visualize the non-linear relationship for 'tax'
ggplot(Boston, aes(x = tax, y = crim)) +
geom_point(alpha = 0.4) +
stat_smooth(method = "lm", formula = y ~ poly(x, 3), color = "red", se = FALSE) +
labs(title = "Non-linear Relationship between Tax and Crime Rate",
x = "Tax", y = "Crime Rate")
# polynomial regression model for 'age' with degree 3
model_age <- lm(crim ~ poly(age, 3), data = Boston)
summary(model_age)
model_age <- lm(crim ~ poly(age, 3), data = Boston)
summary(model_age)
ggplot(Boston, aes(x = age, y = crim)) +
geom_point(alpha = 0.4) +
stat_smooth(method = "lm", formula = y ~ poly(x, 3), color = "blue", se = FALSE) +
labs(title = "Non-linear Relationship between Age and Crime Rate",
x = "Age", y = "Crime Rate")
model_tax <- lm(crim ~ poly(tax, 3), data = Boston)
summary(model_tax)
ggplot(Boston, aes(x = tax, y = crim)) +
geom_point(alpha = 0.4) +
stat_smooth(method = "lm", formula = y ~ poly(x, 3), color = "red", se = FALSE) +
labs(title = "Non-linear Relationship between Tax and Crime Rate",
x = "Tax", y = "Crime Rate")
# Load necessary libraries
library(readr)       # For reading the CSV file
library(tm)          # For text mining and preprocessing
library(SnowballC)   # For stemming
install.packages("SnowballC")
install.packages("tokenizers")
install.packages("quanteda")
install.packages("tm")
install.packages("tm")
# Load necessary libraries
library(readr)       # For reading the CSV file
library(tm)          # For text mining and preprocessing
library(SnowballC)   # For stemming
library(tokenizers)  # For tokenization
library(quanteda)    # For creating Document-Term Matrix
# Step 1: Load the Dataset
data <- read_csv("/mnt/data/bbc.csv")
# Load necessary libraries
library(readr)       # For reading the CSV file
library(tm)          # For text mining and preprocessing
library(SnowballC)   # For stemming
library(tokenizers)  # For tokenization
library(quanteda)    # For creating Document-Term Matrix
# Step 1: Load the Dataset
data <- read_csv("bbc.csv")
# Check the structure of the dataset
str(data)
# Step 2: Preprocessing the Text
# Convert the text to lowercase, remove punctuation and numbers, and perform stemming
corpus <- Corpus(VectorSource(data$text))  # Create corpus
corpus <- tm_map(corpus, content_transformer(tolower))    # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation)               # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)                   # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stop words
corpus <- tm_map(corpus, stemDocument)                    # Perform stemming
# Step 3: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
# Check the dimensions of the matrix
print(dim(dtm))
# Step 4: Remove low-frequency words (15% least frequent terms)
term_frequency <- colSums(as.matrix(dtm))
sorted_terms <- sort(term_frequency, decreasing = TRUE)
# Keep only the top 85% of terms
threshold <- quantile(sorted_terms, 0.85)
dtm_filtered <- dtm[, which(term_frequency >= threshold)]
# Step 5: Display words from the 2205th article with frequency >= 4
article_2205 <- as.matrix(dtm_filtered[2205, ])
feature_vector <- article_2205[article_2205 >= 4]
print(feature_vector)
# Load necessary libraries
library(readr)       # For reading the CSV file
library(tm)          # For text mining and preprocessing
library(SnowballC)   # For stemming
library(tokenizers)  # For tokenization
library(quanteda)    # For creating Document-Term Matrix
# Step 1: Load the Dataset
data <- read.csv("bbc.csv")
# Check the structure of the dataset
str(data)
# Step 2: Preprocessing the Text
# Convert the text to lowercase, remove punctuation and numbers, and perform stemming
corpus <- Corpus(VectorSource(data$text))  # Create corpus
corpus <- tm_map(corpus, content_transformer(tolower))    # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation)               # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)                   # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stop words
corpus <- tm_map(corpus, stemDocument)                    # Perform stemming
# Step 3: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
# Check the dimensions of the matrix
print(dim(dtm))
# Step 4: Remove low-frequency words (15% least frequent terms)
term_frequency <- colSums(as.matrix(dtm))
sorted_terms <- sort(term_frequency, decreasing = TRUE)
# Keep only the top 85% of terms
threshold <- quantile(sorted_terms, 0.85)
dtm_filtered <- dtm[, which(term_frequency >= threshold)]
# Step 5: Display words from the 2205th article with frequency >= 4
article_2205 <- as.matrix(dtm_filtered[2205, ])
feature_vector <- article_2205[article_2205 >= 4]
print(feature_vector)
View(data)
# Load necessary libraries
library(readr)       # For reading the CSV file
library(tm)          # For text mining and preprocessing
library(SnowballC)   # For stemming
library(tokenizers)  # For tokenization
library(quanteda)    # For creating Document-Term Matrix
# Step 1: Load the Dataset
data <- read.csv("bbc.csv")
# Check the structure of the dataset
str(data)
# Step 2: Preprocessing the Text
# Convert the text to lowercase, remove punctuation and numbers, and perform stemming
corpus <- Corpus(VectorSource(data$text))  # Create corpus
corpus <- tm_map(corpus, content_transformer(tolower))    # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation)               # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)                   # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stop words
corpus <- tm_map(corpus, stemDocument)                    # Perform stemming
# Step 3: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
# Check the dimensions of the matrix
print(dim(dtm))
# Step 4: Remove low-frequency words (15% least frequent terms)
term_frequency <- colSums(as.matrix(dtm))
sorted_terms <- sort(term_frequency, decreasing = TRUE)
# Keep only the top 85% of terms
threshold <- quantile(sorted_terms, 0.85)
dtm_filtered <- dtm[, which(term_frequency >= threshold)]
# Step 5: Display words from the 2205th article with frequency >= 4
article_2205 <- as.matrix(dtm_filtered[2205, ])
feature_vector <- article_2205[article_2205 >= 4]
print(feature_vector)
# Load necessary libraries
library(readr)            # For reading data
library(tm)               # For text preprocessing
library(SnowballC)        # For stemming
library(caret)            # For feature selection, train/test split, and evaluation
install.packages("carot")
install.packages("caret")
# Load necessary libraries
library(readr)            # For reading data
library(tm)               # For text preprocessing
library(SnowballC)        # For stemming
library(caret)            # For feature selection, train/test split, and evaluation
library(naivebayes)       # For Multinomial Naive Bayes
install.packages("naivebayes")
# Load necessary libraries
library(readr)            # For reading data
library(tm)               # For text preprocessing
library(SnowballC)        # For stemming
library(caret)            # For feature selection, train/test split, and evaluation
library(naivebayes)       # For Multinomial Naive Bayes
library(nnet)             # For Multinomial Logistic Regression
library(dplyr)            # For data manipulation
# Step 4: Feature Selection - Remove Sparse Terms
dtm <- removeSparseTerms(dtm, 0.99)  # Keep terms that appear in at least 1% of documents
# Convert the DTM to a data frame and add the category labels
df <- as.data.frame(as.matrix(dtm))
df$category <- data$category
# Step 5: Split Data into Training and Test Sets (80-20 Split)
set.seed(123)  # Set seed for reproducibility
train_index <- createDataPartition(df$category, p = 0.8, list = FALSE)  # Stratified split
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Step 6: Train a Multinomial Naive Bayes Classifier
nb_model <- multinomial_naive_bayes(as.matrix(train_data[, -ncol(train_data)]),
train_data$category)
# Predict the categories for the test data
nb_predictions <- predict(nb_model, as.matrix(test_data[, -ncol(test_data)]))
# Step 7: Evaluate the Naive Bayes Model
nb_conf_matrix <- confusionMatrix(nb_predictions, test_data$category)
# Load necessary libraries
library(readr)       # For reading the CSV file
library(tm)          # For text mining and preprocessing
library(SnowballC)   # For stemming
library(tokenizers)  # For tokenization
library(quanteda)    # For creating Document-Term Matrix
# Step 1: Load the Dataset
data <- read.csv("bbc.csv")
# Check the structure of the dataset
str(data)
# Step 2: Preprocessing the Text
# Convert the text to lowercase, remove punctuation and numbers, and perform stemming
corpus <- Corpus(VectorSource(data$text))  # Create corpus
corpus <- tm_map(corpus, content_transformer(tolower))    # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation)               # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)                   # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("english")) # Remove stop words
corpus <- tm_map(corpus, stemDocument)                    # Perform stemming
# Step 3: Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
# Check the dimensions of the matrix
print(dim(dtm))
# Step 4: Remove low-frequency words (15% least frequent terms)
term_frequency <- colSums(as.matrix(dtm))
sorted_terms <- sort(term_frequency, decreasing = TRUE)
# Keep only the top 85% of terms
threshold <- quantile(sorted_terms, 0.85)
dtm_filtered <- dtm[, which(term_frequency >= threshold)]
# Step 5: Display words from the 2205th article with frequency >= 4
article_2205 <- as.matrix(dtm_filtered[2205, ])
feature_vector <- article_2205[article_2205 >= 4]
print(feature_vector)
# Load necessary libraries
library(readr)            # For reading data
library(tm)               # For text preprocessing
library(SnowballC)        # For stemming
library(caret)            # For feature selection, train/test split, and evaluation
library(naivebayes)       # For Multinomial Naive Bayes
library(nnet)             # For Multinomial Logistic Regression
library(dplyr)            # For data manipulation
# Step 4: Feature Selection - Remove Sparse Terms
dtm <- removeSparseTerms(dtm, 0.99)  # Keep terms that appear in at least 1% of documents
# Convert the DTM to a data frame and add the category labels
df <- as.data.frame(as.matrix(dtm))
df$category <- data$category
# Step 5: Split Data into Training and Test Sets (80-20 Split)
set.seed(123)  # Set seed for reproducibility
train_index <- createDataPartition(df$category, p = 0.8, list = FALSE)  # Stratified split
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Step 6: Train a Multinomial Naive Bayes Classifier
nb_model <- multinomial_naive_bayes(as.matrix(train_data[, -ncol(train_data)]),
train_data$category)
# Predict the categories for the test data
nb_predictions <- predict(nb_model, as.matrix(test_data[, -ncol(test_data)]))
# Step 7: Evaluate the Naive Bayes Model
nb_conf_matrix <- confusionMatrix(nb_predictions, test_data$category)
# Load necessary libraries
library(readr)            # For reading data
library(tm)               # For text preprocessing
library(SnowballC)        # For stemming
library(caret)            # For feature selection, train/test split, and evaluation
library(naivebayes)       # For Multinomial Naive Bayes
library(nnet)             # For Multinomial Logistic Regression
library(dplyr)            # For data manipulation
# Step 4: Feature Selection - Remove Sparse Terms
dtm <- removeSparseTerms(dtm, 0.99)  # Keep terms that appear in at least 1% of documents
# Convert the DTM to a data frame and add the category labels
df <- as.data.frame(as.matrix(dtm))
df$category <- data$category
# Step 5: Split Data into Training and Test Sets (80-20 Split)
set.seed(123)  # Set seed for reproducibility
train_index <- createDataPartition(df$category, p = 0.8, list = FALSE)  # Stratified split
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Step 6: Train a Multinomial Naive Bayes Classifier
nb_model <- multinomial_naive_bayes(as.matrix(train_data[, -ncol(train_data)]),
train_data$category)
# Predict the categories for the test data
nb_predictions <- predict(nb_model, as.matrix(test_data[, -ncol(test_data)]))
# Step 7: Ensure Predictions and Actual Categories are Factors with the Same Levels
# Convert both predictions and actual categories to factors with matching levels
nb_predictions <- factor(nb_predictions, levels = levels(test_data$category))
test_data$category <- factor(test_data$category)
# Evaluate the Naive Bayes Model with a Confusion Matrix
nb_conf_matrix <- confusionMatrix(nb_predictions, test_data$category)

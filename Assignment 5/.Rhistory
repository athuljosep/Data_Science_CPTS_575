dtm_df <- as.data.frame(as.matrix(dtm_thres))
# dtm reduction using variance
feature_variances <- apply(dtm_df, 2, var)
variance_threshold <- 0.3
dtm_reduced <- dtm_df[, feature_variances > variance_threshold]
# Print dimensions
print(dim(dtm_df))
print(dim(dtm_reduced))
# splitting into train and test
data_split <- cbind(category = data$category, dtm_reduced)
set.seed(123)
train_index <- createDataPartition(data_split$category, p = 0.8, list = FALSE)
train_data <- data_split[train_index, ]
test_data <- data_split[-train_index, ]
# Separate x & y
train_x <- as.matrix(train_data[, -1])
train_y <- as.factor(train_data$category)
test_x <- as.matrix(test_data[, -1])
# multinomial naive bayes
nb_model <- multinomial_naive_bayes(train_x, train_y)
nb_predictions <- predict(nb_model, newdata = test_x)
nb_predictions <- factor(nb_predictions, levels = levels(train_y))
test_data$category <- factor(test_data$category, levels = levels(train_y))
# confusion matrix
conf_matrix <- confusionMatrix(nb_predictions, test_data$category)
print(conf_matrix)
# Precision and Recall
conf_table <- conf_matrix$table
precision <- diag(conf_table) / rowSums(conf_table)
recall <- diag(conf_table) / colSums(conf_table)
print("Precision by class:")
print(precision)
print("Recall by class:")
print(recall)
# Convert train_x and train_y into a data frame
train_data_df <- as.data.frame(train_x)
train_data_df$category <- train_y  # Add response variable as a column
# Combine predictors and response variable for train and test sets into data frames
train_data_df <- data.frame(train_x, category = train_y)
test_data_df <- data.frame(test_x, category = test_data$category)
# Train the Multinomial Logistic Regression model
log_reg_model <- multinom(category ~ ., data = train_data_df)
# Load necessary library
library(glmnet)
install.packages("glmnet")
# Load necessary library
library(glmnet)
# Convert train_y to a numeric factor for glmnet compatibility
train_y_numeric <- as.numeric(as.factor(train_y)) - 1  # glmnet requires response as numeric starting from 0
# Train a multinomial logistic regression model with glmnet using Ridge regularization (alpha = 0)
log_reg_model <- cv.glmnet(as.matrix(train_x), train_y_numeric, family = "multinomial", alpha = 0)
# Predict on the test set using the best lambda from cross-validation
log_reg_predictions <- predict(log_reg_model, newx = as.matrix(test_x), s = "lambda.min", type = "class")
# Convert predictions back to factors to match the original classes
log_reg_predictions <- factor(log_reg_predictions, labels = levels(train_y))
# Ensure test_y is a factor with the same levels
test_y <- factor(test_data$category, levels = levels(train_y))
# Generate a confusion matrix
conf_matrix_log_reg <- confusionMatrix(log_reg_predictions, test_y)
# Print the confusion matrix
print(conf_matrix_log_reg)
# Calculate precision and recall for each class
conf_table_log_reg <- conf_matrix_log_reg$table
precision_log_reg <- diag(conf_table_log_reg) / rowSums(conf_table_log_reg)
recall_log_reg <- diag(conf_table_log_reg) / colSums(conf_table_log_reg)
# Print precision and recall for each class
cat("Precision by class:\n")
print(precision_log_reg)
cat("Recall by class:\n")
print(recall_log_reg)
# loading libraries
library(quanteda)
library(caret)
library(naivebayes)
library(dplyr)
library(glmnet)
dtm_df <- as.data.frame(as.matrix(dtm_thres))
# dtm reduction using variance
feature_variances <- apply(dtm_df, 2, var)
variance_threshold <- 0.3
dtm_reduced <- dtm_df[, feature_variances > variance_threshold]
# Print dimensions
print(dim(dtm_df))
print(dim(dtm_reduced))
# splitting into train and test
data_split <- cbind(category = data$category, dtm_reduced)
set.seed(123)
train_index <- createDataPartition(data_split$category, p = 0.8, list = FALSE)
train_data <- data_split[train_index, ]
test_data <- data_split[-train_index, ]
# Separate x & y
train_x <- as.matrix(train_data[, -1])
train_y <- as.factor(train_data$category)
test_x <- as.matrix(test_data[, -1])
# multinomial naive bayes
nb_model <- multinomial_naive_bayes(train_x, train_y)
nb_predictions <- predict(nb_model, newdata = test_x)
nb_predictions <- factor(nb_predictions, levels = levels(train_y))
test_data$category <- factor(test_data$category, levels = levels(train_y))
# confusion matrix
conf_matrix <- confusionMatrix(nb_predictions, test_data$category)
print(conf_matrix)
# Precision and Recall
conf_table <- conf_matrix$table
precision <- diag(conf_table) / rowSums(conf_table)
recall <- diag(conf_table) / colSums(conf_table)
print("Precision by class:")
print(precision)
print("Recall by class:")
print(recall)
# Convert train_y to a numeric factor for glmnet compatibility
train_y_numeric <- as.numeric(as.factor(train_y)) - 1  # glmnet requires response as numeric starting from 0
# Train a multinomial logistic regression model with glmnet using Ridge regularization (alpha = 0)
log_reg_model <- cv.glmnet(as.matrix(train_x), train_y_numeric, family = "multinomial", alpha = 0)
# Predict on the test set using the best lambda from cross-validation
log_reg_predictions <- predict(log_reg_model, newx = as.matrix(test_x), s = "lambda.min", type = "class")
# Convert predictions back to factors to match the original classes
log_reg_predictions <- factor(log_reg_predictions, labels = levels(train_y))
# Ensure test_y is a factor with the same levels
test_y <- factor(test_data$category, levels = levels(train_y))
# Generate a confusion matrix
conf_matrix_log_reg <- confusionMatrix(log_reg_predictions, test_y)
# Print the confusion matrix
print(conf_matrix_log_reg)
# Calculate precision and recall for each class
conf_table_log_reg <- conf_matrix_log_reg$table
precision_log_reg <- diag(conf_table_log_reg) / rowSums(conf_table_log_reg)
recall_log_reg <- diag(conf_table_log_reg) / colSums(conf_table_log_reg)
# Print precision and recall for each class
cat("Precision by class:\n")
print(precision_log_reg)
cat("Recall by class:\n")
print(recall_log_reg)
# multinomial logistic regression
train_y_numeric <- as.numeric(as.factor(train_y)) - 1
log_reg_model <- cv.glmnet(as.matrix(train_x), train_y_numeric, family = "multinomial", alpha = 0)
log_reg_predictions <- predict(log_reg_model, newx = as.matrix(test_x), s = "lambda.min", type = "class")
log_reg_predictions <- factor(log_reg_predictions, labels = levels(train_y))
test_y <- factor(test_data$category, levels = levels(train_y))
# confusion matrix
conf_matrix_log_reg <- confusionMatrix(log_reg_predictions, test_y)
print(conf_matrix_log_reg)
# Precision and Recall
conf_table_log_reg <- conf_matrix_log_reg$table
precision_log_reg <- diag(conf_table_log_reg) / rowSums(conf_table_log_reg)
recall_log_reg <- diag(conf_table_log_reg) / colSums(conf_table_log_reg)
print("Precision by class:")
print(precision_log_reg)
print("Recall by class:")
print(recall_log_reg)
# loading libraries
library(quanteda)
library(caret)
library(naivebayes)
library(dplyr)
library(glmnet)
dtm_df <- as.data.frame(as.matrix(dtm_thres))
# dtm reduction using variance
feature_variances <- apply(dtm_df, 2, var)
variance_threshold <- 0.1
dtm_reduced <- dtm_df[, feature_variances > variance_threshold]
# Print dimensions
print(dim(dtm_df))
print(dim(dtm_reduced))
# splitting into train and test
data_split <- cbind(category = data$category, dtm_reduced)
set.seed(123)
train_index <- createDataPartition(data_split$category, p = 0.8, list = FALSE)
train_data <- data_split[train_index, ]
test_data <- data_split[-train_index, ]
# Separate x & y
train_x <- as.matrix(train_data[, -1])
train_y <- as.factor(train_data$category)
test_x <- as.matrix(test_data[, -1])
# multinomial naive bayes
nb_model <- multinomial_naive_bayes(train_x, train_y)
nb_predictions <- predict(nb_model, newdata = test_x)
nb_predictions <- factor(nb_predictions, levels = levels(train_y))
test_data$category <- factor(test_data$category, levels = levels(train_y))
# confusion matrix
conf_matrix <- confusionMatrix(nb_predictions, test_data$category)
print(conf_matrix)
# Precision and Recall
conf_table <- conf_matrix$table
precision <- diag(conf_table) / rowSums(conf_table)
recall <- diag(conf_table) / colSums(conf_table)
print("Precision by class:")
print(precision)
print("Recall by class:")
print(recall)
# multinomial logistic regression
train_y_numeric <- as.numeric(as.factor(train_y)) - 1
log_reg_model <- cv.glmnet(as.matrix(train_x), train_y_numeric, family = "multinomial", alpha = 0)
log_reg_predictions <- predict(log_reg_model, newx = as.matrix(test_x), s = "lambda.min", type = "class")
log_reg_predictions <- factor(log_reg_predictions, labels = levels(train_y))
test_y <- factor(test_data$category, levels = levels(train_y))
# confusion matrix
conf_matrix_log_reg <- confusionMatrix(log_reg_predictions, test_y)
print(conf_matrix_log_reg)
# Precision and Recall
conf_table_log_reg <- conf_matrix_log_reg$table
precision_log_reg <- diag(conf_table_log_reg) / rowSums(conf_table_log_reg)
recall_log_reg <- diag(conf_table_log_reg) / colSums(conf_table_log_reg)
print("Precision by class:")
print(precision_log_reg)
print("Recall by class:")
print(recall_log_reg)
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
article_vector <- as.matrix(dtm_thres[2205, ])
article_vector_filtered <- article_vector[article_vector >= 4]
print(article_vector_filtered)
# loading libraries
library(quanteda)
library(caret)
library(naivebayes)
library(dplyr)
library(glmnet)
dtm_df <- as.data.frame(as.matrix(dtm_thres))
# dtm reduction using variance
feature_variances <- apply(dtm_df, 2, var)
variance_threshold <- 0.1
dtm_reduced <- dtm_df[, feature_variances > variance_threshold]
# print dimensions
print(dim(dtm_df))
print(dim(dtm_reduced))
# loading libraries
library(quanteda)
library(caret)
library(naivebayes)
library(dplyr)
library(glmnet)
dtm_df <- as.data.frame(as.matrix(dtm_thres))
# dtm reduction using variance
feature_variances <- apply(dtm_df, 2, var)
variance_threshold <- 0.1
dtm_reduced <- dtm_df[, feature_variances > variance_threshold]
# print dimensions
print(dim(dtm_df))
print(dim(dtm_reduced))
View(data)
View(article_vector)
View(article_vector)
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
article_vector <- as.matrix(dtm_thres[2205, ])
article_vector_filtered <- article_vector[article_vector >= 4]
print(article_vector_filtered)
View(article_vector)
View(article_vector)
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
article_vector <- as.matrix(dtm_thres[2205, ])
article_vector_filtered <- article_vector[article_vector >= 4]
print(article_vector_filtered)
article_words <- data.frame(
word = rownames(article_vector_filtered),
frequency = article_vector_filtered
)
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
article_vector <- as.matrix(dtm_thres[2205, ])
article_vector_filtered <- article_vector[article_vector >= 4,,drop = FALSE]
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
article_vector <- as.matrix(dtm_thres[2205, ])
article_vector_filtered <- article_vector[article_vector >= 4, ,drop = FALSE]
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
article_vector <- as.matrix(dtm_thres[2205, ])
article_vector_filtered <- article_vector[article_vector >= 4,drop = FALSE]
print(article_vector_filtered)
article_words <- data.frame(
word = rownames(article_vector_filtered),
frequency = article_vector_filtered
)
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
article_vector <- as.matrix(dtm_thres[2205, ])
article_vector_filtered <- article_vector[article_vector >= 4,drop = FALSE]
print(article_vector_filtered)
article_words <- data.frame(
word = rownames(article_vector_filtered),
frequency = as.vector(article_vector_filtered)
)
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
# Extract the row for the 2205th article as a named vector
article_vector <- as.numeric(dtm_thres[2205, ])
names(article_vector) <- colnames(dtm_thres)  # Assign column names as names for words
# Filter words with frequency >= 4
article_vector_filtered <- article_vector[article_vector >= 4]
# Create a data frame with words and frequencies
article_words <- data.frame(
word = names(article_vector_filtered),
frequency = article_vector_filtered
)
# Print the data frame
print(article_words)
# loading libraries
library(readr)
library(tm)
library(SnowballC)
library(quanteda)
# loading dataset
data <- read_csv("bbc.csv", show_col_types = FALSE)
# preprocessing and creating dtm
corpus <- corpus(data$text)
corpus <- tokens(corpus,
what = "word",
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("english")) %>%
tokens_wordstem(language = "en")
dtm <- dfm(corpus)
# top 85% frequency
term_freq <- colSums(as.matrix(dtm))
term_freq <- sort(term_freq, decreasing = TRUE)
threshold <- quantile(term_freq, 0.15)
dtm_thres <- dtm[, term_freq >= threshold]
# 2205th article
article_vector <- as.numeric(dtm_thres[2205, ])
names(article_vector) <- colnames(dtm_thres)
article_vector_filtered <- article_vector[article_vector >= 4]
article_words <- data.frame(
word = names(article_vector_filtered),
frequency = article_vector_filtered
)
print(article_words)
# splitting into train and test
data_split <- cbind(category = data$category, dtm_reduced)
set.seed(123)
train_index <- createDataPartition(
data_split$category,
p = 0.8,
list = FALSE
)
train_data <- data_split[train_index, ]
test_data <- data_split[-train_index, ]
# Separate x & y
train_x <- as.matrix(train_data[, -1])
train_y <- as.factor(train_data$category)
test_x <- as.matrix(test_data[, -1])

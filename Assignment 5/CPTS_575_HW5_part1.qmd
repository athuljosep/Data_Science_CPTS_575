---
title: "Assignment 5"
author: |
  Athul Jose P \\ 
  11867566
dept: "School of Electrical Engineering and Computer Science"
uni: "Washington State University"
subtitle: "CptS 575 Data Science"
geometry: margin=1in
fontsize: 12pt
format: pdf
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{center}\Huge\bfseries}
  - \posttitle{\par\end{center}\vfill}
  - \preauthor{\begin{center}\Large}
  - \postauthor{\par\end{center}\vfill}
  - \predate{\begin{center}\Large}
  - \postdate{\par\end{center}\vfill}
  - \usepackage{fancyhdr}
  - \fancypagestyle{plain}{
      \fancyhf{}
      \renewcommand{\headrulewidth}{0pt}
      \renewcommand{\footrulewidth}{0pt}
      \fancyfoot[C]{\thepage}
    }
  - \pagestyle{empty}
---

```{=tex}
\begin{center}
{\Large Athul Jose P \\ 11867566} \\[0.5cm]
{\large School of Electrical Engineering and Computer Science} \\[0.5cm]
{\large Washington State University} \\[0.5cm]
{\large CptS 575 Data Science} \\[0.5cm]
\end{center}
```

------------------------------------------------------------------------

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
```

\newpage

### **1.a.**

```{r}
# loading dataset
df <- read.csv('winequality-red.csv', header = TRUE)

# linear regression with pH except citric acid
model <- lm(pH ~ . - citric_acid, data = df)

# summary
model_summary <- summary(model)

#print summary
model_summary
```

#### 1.a.i

To identify the predictors which are statistically significant, p-value is taken as a measure. If p-value is less than 0.05, the predictor is considered statistically significant, Otherwise it is not statistically not significant.

Significant Predictors (p \< 0.05):

```{r}
# Extract the coefficients and p-values
coeff_df <- as.data.frame(model_summary$coefficients)

# Filter predictors with p-values < 0.05
significant_pred <- coeff_df[coeff_df$`Pr(>|t|)` < 0.05, ]
print(significant_pred)
```

Non-significant Predictors (p \>= 0.05):

```{r}
# Filter predictors with p-values >= 0.05
non_significant_pred <- coeff_df[coeff_df$`Pr(>|t|)` >= 0.05, ]
print(non_significant_pred)
```

#### 1.a.ii

The coefficient for the free_sulfur_dioxide variable in regression model is 1.653707e-3. In simple terms, for every 1 unit increase in free sulfur dioxide, the predicted pH increases by 0.001653 units, keeping all other variables constant.

### **1.b.**

```{r}
# plot diagnostic plots
par(mfrow = c(2, 2))
plot(model)
```

Residuals vs. Fitted: The residuals are mostly scattered evenly around the horizontal line at zero, suggesting that the linear relationship between the predictors and the response is appropriate. However, there are a few points that stand out slightly (e.g., observations 1313 and 152), which may indicate potential outliers.

Q-Q Residual: Most points lie on the 45-degree line, which suggests that the residuals follow a normal distribution fairly well. However, the points at the far tails (e.g., 1313) deviate slightly from the line, indicating the presence of some outliers.

Scale-Location: The points are fairly evenly spread along the horizontal line, which suggests that the variance of residuals is relatively constant. A few points (e.g., 1313) stand out, which might indicate mild issues with variance at certain fitted values.

Residuals vs Leverage: Most points have low leverage, clustered to the left near zero. Observations 152 and 292 as high leverage points

### **1.c.**

```{r}
# Model 1: Interaction between sulphates and residual sugar
model1 <- lm(alcohol ~ sulphates * residual_sugar, data = df)
summary(model1)
```

No significant interactions are found between sulphates and residual_sugar in this model. Additionally, the individual predictors themselves (sulphates and residual_sugar) are not statistically significant either.

```{r}
# Model 2: Interaction between density and total sulfur dioxide
model2 <- lm(alcohol ~ density * total_sulfur_dioxide, data = df)
summary(model2)
```

The interaction effect between density and total_sulfur_dioxide is not statistically significant (p-value = 0.262). density has a strong, statistically significant effect on alcohol, but total_sulfur_dioxide does not.

```{r}
# Model 3: Interaction between volatile acidity and chlorides
model3 <- lm(alcohol ~ volatile_acidity * chlorides, data = df)
summary(model3)
```

The interaction effect between volatile_acidity and chlorides is not statistically significant (p-value = 0.86006). However, both volatile_acidity and chlorides individually have statistically significant effects on alcohol content.

\newpage

### **2.a.**

```{r}
# loading MASS library and Boston dataset
library(MASS)
data("Boston")

# predictors
predictors <- setdiff(names(Boston), "crim")
predictors
```

```{r}
# fit a simple linear regression model
model_zn      <- lm(crim ~ zn, data = Boston)
model_indus   <- lm(crim ~ indus, data = Boston)
model_chas    <- lm(crim ~ chas, data = Boston)
model_nox     <- lm(crim ~ nox, data = Boston)
model_rm      <- lm(crim ~ rm, data = Boston)
model_age     <- lm(crim ~ age, data = Boston)
model_dis     <- lm(crim ~ dis, data = Boston)
model_rad     <- lm(crim ~ rad, data = Boston)
model_tax     <- lm(crim ~ tax, data = Boston)
model_ptratio <- lm(crim ~ ptratio, data = Boston)
model_black   <- lm(crim ~ black, data = Boston)
model_lstat   <- lm(crim ~ lstat, data = Boston)
model_medv    <- lm(crim ~ medv, data = Boston)
```

### **2.b.**

```{r}
summary(model_nox)
```

A positive and significant relationship suggests that areas with higher pollution levels (nox) tend to have higher crime rates. This makes sense since higher nox concentrations are often found in urban and industrial areas, which may also have higher crime rates compared to suburban or rural areas.

```{r}
summary(model_chas)
```

The negative coefficient suggests a potential trend where areas near the Charles River might have lower crime rates. However, this effect is not statistically significant. Even though living near the river might indicate more desirable neighborhoods, in this analysis, the effect of chas on crim is not strong enough to be considered statistically significant.

```{r}
summary(model_rm)
```

There is a significant negative relationship between the average number of rooms (rm) and per capita crime rate (crim). This suggests that areas with larger homes tend to have lower crime rates, likely reflecting the fact that wealthier or suburban areas generally experience less crime.

```{r}
summary(model_dis)
```

The model shows a statistically significant negative relationship between dis and crim. As distance from employment centers increases, crime rates tend to decrease, suggesting that suburban or rural areas have lower crime rates compared to urban areas.

```{r}
summary(model_medv)
```

There is a statistically significant negative relationship between the median value of homes (medv) and the crime rate (crim). As the median value of homes increases, the crime rate decreases, suggesting that wealthier neighborhoods tend to have lower crime rates.

Generally, the analysis highlights that crime tends to increse with urbanization (pollution) and decrease with affluence and suburbanization (more rooms, higher home values, greater distance from city centers)

### **2.c.**

```{r}
#  multiple regression model with all predictors
model_full <- lm(crim ~ ., data = Boston)

# print summary
summary(model_full)
```

The multiple linear regression model for predicting the per capita crime rate (crim) reveals several significant predictors. Highway accessibility (rad) shows a strong positive relationship with crime, indicating that areas with greater highway access tend to have higher crime rates. Conversely, distance to employment centers (dis) and zoned residential land (zn) exhibit significant negative effects, suggesting that suburban or rural areas experience lower crime. Additionally, higher property values (medv) and greater proportion of Black residents (black) are associated with lower crime rates. These findings align with general expectations that affluent, suburban areas with fewer urban stressors experience less crime.

However, some variables, such as pollution levels (nox) and percentage of lower-status individuals (lstat), show marginal significance, suggesting complex or weak relationships with crime. Predictors like proximity to the Charles River (chas), average number of rooms (rm), tax rates, and pupil-teacher ratio (ptratio) are not statistically significant, indicating limited direct influence on crime rates. The model explains 45.4% of the variance in the crime rate, providing a moderate fit, with the overall model being statistically significant. These results highlight how a combination of urbanization, infrastructure, property values, and social factors contribute to crime patterns.

For these five predictors zn, dis, rad, black, and medv, null hypothesis can be rejected meaning they have a statistically significant effect on the crime rate. All other predictors in the model have p-values ≥ 0.05, so we fail to reject the null hypothesis for those predictors.

### **2.d.**

```{r}
library(ggplot2)

# exclude intercept
multiple_pvalues <- summary(model_full)$coefficients[-1, "Pr(>|t|)"]

# get p values from simple
get_simple_pvalue <- function(predictor) {
  model <- lm(as.formula(paste("crim ~", predictor)), data = Boston)
  summary(model)$coefficients[2, "Pr(>|t|)"]
}
predictors <- names(Boston)[names(Boston) != "crim"]
simple_pvalues <- sapply(predictors, get_simple_pvalue)

# collect data
comparison_df <- data.frame(
  Predictor = predictors,
  Simple_Regression = simple_pvalues,
  Multiple_Regression = multiple_pvalues
)

# df for plotting
comparison_df_long <- data.frame(
  Predictor = rep(comparison_df$Predictor, 2),
  Model = c(rep("Simple Regression", length(predictors)), 
            rep("Multiple Regression", length(predictors))),
  p_value = c(comparison_df$Simple_Regression, comparison_df$Multiple_Regression)
)

# plotting bar plot
ggplot(comparison_df_long, aes(x = Predictor, y = p_value, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_log10() +
  labs(title = "Comparison of P-values: Simple vs. Multiple Regression",
       y = "P-value (log scale)", x = "Predictor")
```

The graph reveals that some predictors retain their significance across both simple and multiple regression models (e.g., rad), while others (e.g., rm, zn, tax) lose significance when other predictors are included. This highlights the importance of using multiple regression to account for overlapping influences among variables, providing a clearer picture of which factors have a direct and unique impact on the crime rate.

### **2.e.**

**Polynomial regression model for age**

```{r}
model_age <- lm(crim ~ poly(age, 3), data = Boston)
summary(model_age)
```

There is strong evidence of non-linear association between age and crim due to the significance of the quadratic and cubic terms. This suggests that changes in the proportion of older houses have a complex relationship with crime, possibly reflecting different social or economic dynamics across neighborhoods of varying age compositions

**Polynomial regression model for tax**

```{r}
model_tax <- lm(crim ~ poly(tax, 3), data = Boston)
summary(model_tax)
```

There is moderate evidence of non-linear association between tax and crim, primarily driven by the significant quadratic term. This suggests that the relationship between property tax rate and crime is not purely linear—there may be regions where the increase in crime with higher taxes is more or less pronounced. However, since the cubic term is not significant, a quadratic model is likely the best fit for capturing this non-linear trend.

### **3.a.**

Logistic function is given by

$$
p(Y = 1 | X_1, X_2, X_3) = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3}}{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3}}
$$

Given

$X_1 = 32, X_2 = 3, X_3 = 11$

$\beta_0 = -8, \beta_1 = 0.1, \beta_2 = 1, \beta_3 = -0.04$

Then

$$
p(Y = 1 | X_1, X_2, X_3) = \frac{e^{-8 + 0.1*32 + 1*3 + -0.04*11}}{1 + e^{-8 + 0.1*32 + 1*3 + -0.04*1}} = 0.096
$$

The model estimates that the student has approximately a 9.6% chance of receiving an A in the class based on their study hours, GPA, and sleep quality index.

### **3.b.**

Let

$$z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3$$ then

$$
p(Y = 1 | X_1, X_2, X_3) = \frac{e^z}{1 + e^z}
$$

Given

$$
p(Y = 1 | X_1, X_2, X_3) = 65\% = 0.65
$$

$$
\frac{e^z}{1 + e^z} = 0.65
$$

$$
(1-0.65)e^z = 0.65 
$$

$$
e^z = \frac{0.65}{0.35} = 1.857
$$

$$
z = ln(1.857) = 0.619
$$

Given

$X_2 = 3, X_3 = 11$

$\beta_0 = -8, \beta_1 = 0.1, \beta_2 = 1, \beta_3 = -0.04$

Then

$$-8 + 0.1(X_1) + 1*3 + -0.04*11= 0.619$$

$$
X_1 = \frac{0.619+8-3+0.44}{0.1} = 60.59
$$

The student would need to study approximately 61 hours to achieve a 65% chance of receiving an A in the class.

### **3.c.**

Let

$$z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3$$ then

$$ p(Y = 1 | X_1, X_2, X_3) = \frac{e^z}{1 + e^z} $$

Given

$$ p(Y = 1 | X_1, X_2, X_3) = 60\% = 0.6 $$

$$ \frac{e^z}{1 + e^z} = 0.6 $$

$$ (1-0.6)e^z = 0.6  $$

$$ e^z = \frac{0.6}{0.4} = 1.5 $$

$$ z = ln(1.5) = 0.405 $$

Given

$X_2 = 3, X_3 = 3$

$\beta_0 = -8, \beta_1 = 0.1, \beta_2 = 1, \beta_3 = -0.04$

Then

$$-8 + 0.1(X_1) + 1*3 + -0.04*3= 0.405$$

$$ X_1 = \frac{0.405+8-3+0.12}{0.1} = 55.25 $$

A student with a GPA of 3.0 and a PSQI score of 3 would need to study approximately 55 hours to have a 60% chance of receiving an A in the class.
